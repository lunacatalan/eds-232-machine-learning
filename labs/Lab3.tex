% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={HerschenfeldCatalan\_Lab3},
  pdfauthor={Luna Herschenfeld-Catalan},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{HerschenfeldCatalan\_Lab3}
\author{Luna Herschenfeld-Catalan}
\date{2024-01-31}

\begin{document}
\maketitle

\hypertarget{lab-3-predicting-the-age-of-abalone}{%
\subsection{Lab 3: Predicting the age of
abalone}\label{lab-3-predicting-the-age-of-abalone}}

Abalones are marine snails. Their flesh is widely considered to be a
desirable food, and is consumed raw or cooked by a variety of cultures.
The age of abalone is determined by cutting the shell through the cone,
staining it, and counting the number of rings through a microscope -- a
boring and time-consuming task. Other measurements, which are easier to
obtain, are used to predict the age.

The data set provided includes variables related to the sex, physical
dimensions of the shell, and various weight measurements, along with the
number of rings in the shell. Number of rings is the stand-in here for
age.

\hypertarget{data-exploration}{%
\subsubsection{Data Exploration}\label{data-exploration}}

Pull the abalone data from Github and take a look at it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abdat}\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"https://raw.githubusercontent.com/MaRo406/eds{-}232{-}machine{-}learning/main/data/abalone{-}data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## New names:
## Rows: 4177 Columns: 10
## -- Column specification
## -------------------------------------------------------- Delimiter: "," chr
## (1): Sex dbl (9): ...1, Length, Diameter, Height, Whole_weight, Shucked_weight,
## Visce...
## i Use `spec()` to retrieve the full column specification for this data. i
## Specify the column types or set `show_col_types = FALSE` to quiet this message.
## * `` -> `...1`
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(abdat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 4,177
## Columns: 10
## $ ...1           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ~
## $ Sex            <chr> "M", "M", "F", "M", "I", "I", "F", "F", "M", "F", "F", ~
## $ Length         <dbl> 0.455, 0.350, 0.530, 0.440, 0.330, 0.425, 0.530, 0.545,~
## $ Diameter       <dbl> 0.365, 0.265, 0.420, 0.365, 0.255, 0.300, 0.415, 0.425,~
## $ Height         <dbl> 0.095, 0.090, 0.135, 0.125, 0.080, 0.095, 0.150, 0.125,~
## $ Whole_weight   <dbl> 0.5140, 0.2255, 0.6770, 0.5160, 0.2050, 0.3515, 0.7775,~
## $ Shucked_weight <dbl> 0.2245, 0.0995, 0.2565, 0.2155, 0.0895, 0.1410, 0.2370,~
## $ Viscera_weight <dbl> 0.1010, 0.0485, 0.1415, 0.1140, 0.0395, 0.0775, 0.1415,~
## $ Shell_weight   <dbl> 0.150, 0.070, 0.210, 0.155, 0.055, 0.120, 0.330, 0.260,~
## $ Rings          <dbl> 15, 7, 9, 10, 7, 8, 20, 16, 9, 19, 14, 10, 11, 10, 10, ~
\end{verbatim}

\hypertarget{data-splitting}{%
\subsubsection{Data Splitting}\label{data-splitting}}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 1}}. Split the data into training and test
  sets. Use a 70/30 training/test split.
\end{itemize}

We'll follow our text book's lead and use the caret package in our
approach to this task. We will use the glmnet package in order to
perform ridge regression and the lasso. The main function in this
package is glmnet(), which can be used to fit ridge regression models,
lasso models, and more. In particular, we must pass in an x matrix of
predictors as well as a y outcome vector , and we do not use the yâˆ¼x
syntax.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# do the inital split}
\NormalTok{ab\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(abdat,}
                          \AttributeTok{prop =} \FloatTok{0.7}\NormalTok{)}

\CommentTok{\# split data into test and training data }
\NormalTok{ab\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(ab\_split)}
\NormalTok{ab\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(ab\_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{fit-a-ridge-regression-model}{%
\subsubsection{Fit a ridge regression
model}\label{fit-a-ridge-regression-model}}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 2}}. Use the model.matrix() function to create
  a predictor matrix, x, and assign the Rings variable to an outcome
  vector, y.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(Rings }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\CommentTok{\# make all variables predictors}
                  \AttributeTok{data =}\NormalTok{ ab\_train)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }

\CommentTok{\# assign Predictor variable}
\NormalTok{Y }\OtherTok{\textless{}{-}}\NormalTok{ ab\_train}\SpecialCharTok{$}\NormalTok{Rings}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 3}}. Fit a ridge model (controlled by the alpha
  parameter) using the glmnet() function. Make a plot showing how the
  estimated coefficients change with lambda. (Hint: You can call plot()
  directly on the glmnet() objects).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#fit a ridge model, passing X,Y,alpha to glmnet()}
\NormalTok{ridge }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ X,}
  \AttributeTok{y =}\NormalTok{ Y,}
  \AttributeTok{alpha =} \DecValTok{0} \CommentTok{\# rigde function}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{plot}\NormalTok{(}\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{) }\CommentTok{\# plot with lambda on x axis}
\end{Highlighting}
\end{Shaded}

\includegraphics{Lab3_files/figure-latex/unnamed-chunk-3-1.pdf}

\hypertarget{using-k-fold-cross-validation-resampling-and-tuning-our-models}{%
\subsubsection{\texorpdfstring{Using \emph{k}-fold cross validation
resampling and tuning our
models}{Using k-fold cross validation resampling and tuning our models}}\label{using-k-fold-cross-validation-resampling-and-tuning-our-models}}

In lecture we learned about two methods of estimating our model's
generalization error by resampling, cross validation and bootstrapping.
We'll use the \emph{k}-fold cross validation method in this lab. Recall
that lambda is a tuning parameter that helps keep our model from
over-fitting to the training data. Tuning is the process of finding the
optima value of lambda.

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 4}}. This time fit a ridge regression model and
  a lasso model, both with using cross validation. The glmnet package
  kindly provides a cv.glmnet() function to do this (similar to the
  glmnet() function that we just used). Use the alpha argument to
  control which type of model you are running. Plot the results.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{\# cross validation }
\NormalTok{ab\_ridge }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{( }
  \AttributeTok{x =}\NormalTok{ X,}
  \AttributeTok{y =}\NormalTok{ Y,}
  \AttributeTok{alpha =} \DecValTok{0}
\NormalTok{)}

\CommentTok{\# plot ridge regression}
\NormalTok{ab\_ridge }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{plot}\NormalTok{(}\AttributeTok{main =} \StringTok{"Ridge penalty"}\NormalTok{)}

\CommentTok{\# cross validation }
\NormalTok{ab\_lasso }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ X,}
  \AttributeTok{y =}\NormalTok{ Y,}
  \AttributeTok{alpha =} \DecValTok{1}
\NormalTok{) }

\CommentTok{\# plot lasso regression}
\NormalTok{ab\_lasso }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{plot}\NormalTok{(}\AttributeTok{main =} \StringTok{"Lasso penalty"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Lab3_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 5}}. Interpret the graphs. What is being
  displayed on the axes here? How does the performance of the models
  change with the value of lambda?
\end{itemize}

The graphs are showing model performance as a function of lambda. On the
x axis, the graph is showing the value of lambda. On the y-axis is
Mean-squared error (MSE). For both graphs, as lambda increases, the MSE
increases, and therefore the model performance decreases. The first
dotted line represents the value of lambda that gives you the lowest
mean squared error. The second dotted line identifies the value of
lambda where the model has the fewest number of variables.

For the Ridge penalty model, the lambda value that minimizes MSE and
where the model has the fewest number of variables is -1. For the Lasso
penalty model that minimizes the MSE and has the fewest number of
variables is a large range between -3 and less than -7.

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 6}}. Inspect the ridge model object you created
  with cv.glmnet(). The \$cvm column shows the MSEs for each CV fold.
  What is the minimum MSE? What is the value of lambda associated with
  this MSE minimum?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# minimum MSE }
\NormalTok{min\_mse }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(ab\_ridge}\SpecialCharTok{$}\NormalTok{cvm)}

\CommentTok{\# print lambda when MSE is minimized}
\NormalTok{min\_lam }\OtherTok{\textless{}{-}}\NormalTok{ ab\_ridge}\SpecialCharTok{$}\NormalTok{lambda.min}
\end{Highlighting}
\end{Shaded}

The minimum MSE for the ridge model is 5.0605839 and the lambda value
associated with the MSE minimum is 0.2021107.

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 7}}. Do the same for the lasso model. What is
  the minimum MSE? What is the value of lambda associated with this MSE
  minimum?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# minimum MSE }
\NormalTok{min\_mse\_2 }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(ab\_lasso}\SpecialCharTok{$}\NormalTok{cvm)}

\CommentTok{\# print lambda when MSE is minimized}
\NormalTok{min\_lam\_2 }\OtherTok{\textless{}{-}}\NormalTok{ ab\_lasso}\SpecialCharTok{$}\NormalTok{lambda.min}
\end{Highlighting}
\end{Shaded}

The minimum MSE for the lasso model is 4.7796683 and the lambda value
associated with the MSE minimum is 0.0011838.

Data scientists often use the ``one-standard-error'' rule when tuning
lambda to select the best model. This rule tells us to pick the most
parsimonious model (fewest number of predictors) while still remaining
within one standard error of the overall minimum cross validation error.
The cv.glmnet() model object has a column that automatically finds the
value of lambda associated with the model that produces an MSE that is
one standard error from the MSE minimum (\$lambda.1se).

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 8.}} Find the number of predictors associated
  with this model (hint: the \$nzero is the \# of predictors column).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# when looking at the predictors column, select the lambda that produces MSE 1se from the minimum}
\NormalTok{lasso\_pred }\OtherTok{\textless{}{-}}\NormalTok{ ab\_lasso}\SpecialCharTok{$}\NormalTok{nzero[ab\_lasso}\SpecialCharTok{$}\NormalTok{lambda }\SpecialCharTok{==}\NormalTok{ ab\_lasso}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se]}

\CommentTok{\# when looking at the predictors column, select the lambda that produces MSE 1se from the minimum}
\NormalTok{ridge\_pred }\OtherTok{\textless{}{-}}\NormalTok{ ab\_ridge}\SpecialCharTok{$}\NormalTok{nzero[ab\_ridge}\SpecialCharTok{$}\NormalTok{lambda }\SpecialCharTok{==}\NormalTok{ ab\_ridge}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se]}
\end{Highlighting}
\end{Shaded}

The number of predictors in the lasso model is 7, which is less than the
total number because it performs selection on the predictors to minimize
the number needed in the model. The number of predictors in the ridge
model is 10, and it includes all the columns in the data set. This makes
sense since ridge regression does not perform feature selection and
keeps all predictors.

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 9}.} Which regularized regression worked better
  for this task, ridge or lasso? Explain your answer.
\end{itemize}

For this task, the lasso model works slightly better for predicting the
age of abalone. This can be determined by comparing the MSE for the
ridge and lasso. The lasso model has a smaller MSE output (lasso:
4.7796683 vs ridge: 5.0605839). Also, the lasso uses less coefficients
(6 vs 10), which makes it a simpler model. The more simple model and the
lower MSE makes the lasso better than the ridge model to predict the age
of abalone.

Attempt at running model on test data abd evaluatin results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{X\_test }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(Rings }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ ab\_test)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }\CommentTok{\# make all variables predictors}

\CommentTok{\# assign Predictor variable}
\NormalTok{Y\_test }\OtherTok{\textless{}{-}}\NormalTok{ ab\_test}\SpecialCharTok{$}\NormalTok{Rings}

\CommentTok{\# cross validation for lasso }
\NormalTok{bestlam\_lasso }\OtherTok{\textless{}{-}}\NormalTok{ ab\_lasso}\SpecialCharTok{$}\NormalTok{lambda.min }\CommentTok{\# Select lamda that minimizes training MSE}
\NormalTok{lasso\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ab\_lasso, }\AttributeTok{s =}\NormalTok{ bestlam\_lasso, }\AttributeTok{newx =}\NormalTok{ X\_test) }\CommentTok{\# Use best lambda to predict test data}
\NormalTok{lasso\_mse }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((lasso\_pred }\SpecialCharTok{{-}}\NormalTok{ Y\_test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\CommentTok{\# Calculate test MSE}

\CommentTok{\# cross validation for ridge }
\NormalTok{bestlam\_ridge }\OtherTok{\textless{}{-}}\NormalTok{ ab\_ridge}\SpecialCharTok{$}\NormalTok{lambda.min }\CommentTok{\# Select lamda that minimizes training MSE}
\NormalTok{ridge\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ab\_ridge, }\AttributeTok{s =}\NormalTok{ bestlam\_ridge, }\AttributeTok{newx =}\NormalTok{ X\_test) }\CommentTok{\# Use best lambda to predict test data}
\NormalTok{ridge\_mse }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((ridge\_pred }\SpecialCharTok{{-}}\NormalTok{ Y\_test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\CommentTok{\# Calculate test MSE}
\end{Highlighting}
\end{Shaded}


\end{document}
