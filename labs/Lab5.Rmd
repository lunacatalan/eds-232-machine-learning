---
title: "Lab5"
author: "Luna Herschenfeld-Catal√°n"
date: "2023-02-07"
output: html_document
---

This week's lab is a musical lab. You'll be requesting data from the Spotify API and using it to build k-nearest neighbor and decision tree models.

In order to use the Spotify API you must have a Spotify account. If you don't have one, sign up for a free one here: <https://www.spotify.com/us/signup>

Once you have an account, go to Spotify for developers (<https://developer.spotify.com/>) and log in. Click the green "Create a Client ID" button to fill out the form to create an app so you can access the API.

On your developer dashboard page, click on the new app you just created. Go to Settings -\> Basic Information and you will find your Client ID . Click "View client secret" to access your secondary Client ID. Scroll down to Redirect URIs and enter: <http://localhost:1410/>

You have two options for completing this lab.

**Option 1**: **Classify by users**. Build models that predict whether a given song will be in your collection vs. a partner in class. This requires that you were already a Spotify user so you have enough data to work with. You will download your data from the Spotify API and then exchange with another member of class.

```{r message=FALSE}
library(spotifyr) #API interaction
library(tidyverse)
library(tidymodels)

# for decision trees
library(rpart)
library(caret)
library(rpart.plot)

# for bag tree
library(baguette)
```

Client ID and Client Secret are required to create and access token that is required to interact with the API. You can set them as system values so we don't have to do provide them each time.

```{r access_API}

Sys.setenv(SPOTIFY_CLIENT_ID = '4e6b4f1c38794febb993a3b3b16615b4')
Sys.setenv(SPOTIFY_CLIENT_SECRET = '5a3e4c703b274d4089b67f5c98c0d58e')

authorization_code <- get_spotify_authorization_code(scope = scopes()[c(1:19)]) #sets an authorization code that you'll need to provide for certain get_ functions via my_tracks <- get_my_saved_tracks(authorization = authorization_code)

access_token <- get_spotify_access_token() #takes ID and SECRET, sends to Spotify and receives an access token
```

## **Option 1: Data Preparation**

You can use get_my_saved_tracks() to request all your liked tracks. It would be good if you had at least 150-200 liked tracks so the model has enough data to work with. If you don't have enough liked tracks, you can instead use get_my_recently_played(), and in that case grab at least 500 recently played tracks if you can.

The Spotify API returns a dataframe of tracks and associated attributes. However, it will only return up to 50 (or 20) tracks at a time, so you will have to make multiple requests. Use a function to combine all your requests in one call.

Once you have your tracks, familiarize yourself with this initial dataframe. You'll need to request some additional information for the analysis. If you give the API a list of track IDs using get_track_audio_features(), it will return an audio features dataframe of all the tracks and some attributes of them.

These track audio features are the predictors we are interested in, but this dataframe doesn't have the actual names of the tracks. Append the 'track.name' column from your favorite tracks database.

Find a class mate whose data you would like to use. Add your partner's data to your dataset. Create a new column that will contain the outcome variable that you will try to predict. This variable should contain two values that represent if the track came from your data set or your partner's.


###Data Exploration (both options)

Let's take a look at your data. Do some exploratory summary stats and visualization.

For example: What are the most danceable tracks in your dataset? What are some differences in the data between users (Option 1) or genres (Option 2)?

```{r}
# Has to iterate 15 times
# index increases by 20

index_seq = seq(0, 300, 16)
saved_tracks_df = data.frame()

# create function to access 300 saved tracks
saved_tracks <- function(AC) {
  
  index_seq = seq(0, 300, 16)
  
  for (track in index_seq) {
    
    saved_tracks <- get_my_saved_tracks(offset = (track),
                                           authorization = AC)
    
    saved_tracks_df <- rbind(saved_tracks_df, saved_tracks)
    
  }
  
  saved_tracks_df
}

saved_df <- saved_tracks(authorization_code)

```

### Combine Data

```{r}
sam_df <- read_csv(here::here("labs", "data", "sam_audio.csv")) %>% 
  select(-track.name) %>% 
  mutate(partner_id = "Sam")
  
artists <- saved_df %>% 
  select(track.name, track_id = track.id) %>% 
  slice_head(n = 200)

luna_df <- get_track_audio_features(saved_df$track.id[1:100]) %>% 
  rbind(get_track_audio_features(saved_df$track.id[101:200])) %>% 
  mutate(partner_id = "Luna") 

luna_artists <- luna_df %>% 
  cbind(artists)

features_df <- rbind(luna_df, sam_df) %>% 
  mutate_if(is.ordered, .funs = factor, ordered = F) %>% 
  select(-type, -id, -uri, -track_href, -analysis_url, -time_signature) %>% 
  mutate(partner_id = as.factor(partner_id))
  
```

### Visualizations
```{r}

ggplot() +
  geom_point(data = features_df,
             aes(x = danceability, y = energy, 
                 color = partner_id)) +
  theme_bw()

ggplot() +
  geom_point(data = features_df,
             aes(x = danceability, y = acousticness, 
                 color = partner_id)) +
  theme_bw()

slice <- luna_artists %>% 
  slice_max(order_by = loudness, 
             n = 10)

ggplot(data = luna_artists) +
  geom_point(aes(x = danceability, y = energy, 
                 size = loudness, alpha = 0.5,
                 color = "red")) +
  geom_text(data = slice,
    aes(x = danceability, y = energy,
        label = track.name),
    size = 3,
    min.segment.length = 0, 
    seed = 10, 
    box.padding = 0.5,
    max.overlaps = Inf,
    arrow = arrow(length = unit(0.010, "npc")),
    #nudge_x = .15,
    #nudge_y = .5,
    color = "black") +
  theme_bw()

```


# **Modeling**

Create competing models that predict whether a track belongs to:

Option 1. you or your partner's collection:

```{r eval = FALSE, include = FALSE}
# clean the data 
saved_artists <- saved_df[[2]]
artist_df = data.frame()

## These do the same thing:
# get artist names from the column of dataframes
for (artist in saved_artists) {
  artists <- artist
  artist_df <- rbind(artist_df, artists)
}

saved_artist_df <- bind_rows(saved_artists, .id = "artist_id")

saved_clean_luna <- saved_df %>% 
  mutate(artist_id = as.character(row_number())) %>% 
  relocate(artist_id, .before = added_at) %>% 
  left_join(saved_artist_df) %>% 
  select() # select the columns we want!
```

Go through the modeling process for each model.

## Preprocessing\
You can use the same recipe for all the models you create.

```{r, message = FALSE}
set.seed(279)
# split the data
feature_split <- initial_split(features_df)

feature_train <- training(feature_split)
feature_test <- testing(feature_split)

# make recipe for variables 
feature_recipe <- recipe(partner_id ~ ., data = feature_train) %>% 
  # create dummy variables for all nominal predictors
  step_dummy(all_nominal_predictors()) %>% # same as saying all_predictors() and -all_outcomes()
  step_normalize(all_numeric_predictors()) # for scaling purposes

```

Resampling. Make sure to use appropriate resampling to select the best version created by each algorithm.

Tuning. Find the best values for each hyperparameter (within a reasonable range).


### 1.  k-nearest neighbor (Week 5)
```{r}
set.seed(279)
# set the neighbors to tuning so that it selects the best value of neighbors to increase accuracy
feature_knn_tune <- nearest_neighbor(neighbors = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")

# create workflow
workflow_tune <- workflow() %>% 
  add_model(feature_knn_tune) %>% 
  add_recipe(feature_recipe)

# cross validation 
set.seed(279)
# 5-fold CV on the training dataset (instead of 10 for in-class demo)
cv_folds <- feature_train %>% 
  vfold_cv(v = 5)

# Resampling and do cross validation 
feature_res <- 
  workflow_tune %>% 
  tune_grid(
    resamples = cv_folds, # defined above 
    grid = 8
    )

#autoplot(feature_res)

feature_final <- finalize_workflow(workflow_tune, # give workflow
                               
                               # select_best gives less information;
                               select_best(feature_res)) 
                                           #metric = "roc_auc")) # just the metric


# fit the model to the training data
train_fit_knn <- fit(feature_final, feature_train)
train_knn <- last_fit(feature_final, feature_split) # use the split

feature_predict_knn <- predict(train_fit_knn, feature_test) %>% 
  bind_cols(feature_test) %>% 
  mutate(partner_id = as.factor(partner_id))

# with probabilities -- use the one above
feature_predict_knn_prob <- predict(train_fit_knn, feature_test, type = "prob") %>% 
  bind_cols(feature_test) %>% 
  mutate(partner_id = as.factor(partner_id))

knn_accuracy = accuracy(feature_predict_knn, truth = partner_id,
         estimate = .pred_class)
```


### 2.  Decision tree (Week 5)

```{r eval = FALSE, include = FALSE}
feature_tree <- rpart(
  formula = partner_id ~ ., 
  data = feature_train,
  method = "class",
  control = list(cp = 0, xval = 10)
)

plot(feature_tree)
plotcp(feature_tree)
feature_tree$cptable

rpart.plot(feature_tree)


# select the CP that selects the minimum xerror
bestcp <- feature_tree$cptable[which.min(feature_tree$cptable[,"xerror"]),"CP"]

# prune the tree 
feature_prune <- prune(feature_tree, cp = bestcp)

prp(feature_prune, faclen = 0, cex = 0.8, extra = 1)

predict(feature_prune, data = feature_test, type = "prob")

feature_predict_dt <- predict(feature_prune, 
                              feature_test, type = "class") %>% 
  bind_cols(feature_test) %>% 
  rename(.pred_class = ...1)

accuracy(feature_predict_dt, truth = partner_id,
         estimate = .pred_class)

sensitivity(feature_predict, truth = partner_id, estimate = .pred_class)
specificity(feature_predict, truth = partner_id, estimate = .pred_class)

# caret cross validation 
feature_tree2 <- train(
  partner_id ~ .,
  data = feature_train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 20
)

ggplot(feature_tree2)

# look at the importance of the features
vip::vip(feature_tree2, # model 
         num_features = 10, 
         geom = "point")

```

```{r}
set.seed(279)
tree_spec_tune <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

# to tune; need grid of parameters that are going to be in the model 
tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(),
                          levels = 5)

# make workflow 
wf_tree_tune <- workflow() %>% 
  add_recipe(feature_recipe) %>% 
  add_model(tree_spec_tune)


#set up k-fold cv. This can be used for all the algorithms
feature_cv <- feature_train %>% 
  vfold_cv(v = 10) # 10 cross validation folds

# so that this doesnt take long to run, do in sequence
doParallel::registerDoParallel() #build trees in parallel

# Resampling 
system.time( # measure how long it takes to run
  tree_rs <- tune_grid(
    wf_tree_tune, # add workflow with model 
    resamples = feature_cv,
    grid = tree_grid,
    metrics = metric_set(accuracy) # look at accuracy
  )
)

# show_best(tree_rs, n = 3)

# make a final workflow that selects the best values 
final_tree <- finalize_workflow(wf_tree_tune, 
                                select_best(tree_rs))

final_tree_result <- last_fit(final_tree,
                              feature_split) # use split

```


```{r eval = FALSE, include = TRUE}
## Exploration 
#tree_rs

# plot the accuracy
tree_rs %>% autoplot() +
  theme_bw()

final_tree

## compare these to the way you did the tree above
final_tree_result$.predictions

# the testing accuracy and auc for the test data
final_tree_result$.metrics
```


### 3.  bagged tree (Week 6)
    -   bag_tree()
    -   Use the "times =" argument when setting the engine during model specification to specify the number of trees. The rule of thumb is that 50-500 trees is usually sufficient. The bottom of that range should be sufficient here.

```{r}
set.seed(279)
# tune the model 
bag_tree_spec_tune <- bag_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>% 
  set_engine("rpart",
             times = 60) %>% 
  set_mode("classification")

# to tune; need grid of parameters that are going to be in the model 
bag_tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(),
                          levels = 5)

# make workflow 
wf_bag_tree_tune <- workflow() %>% 
  add_recipe(feature_recipe) %>% 
  add_model(bag_tree_spec_tune)

#set up k-fold cv. This can be used for all the algorithms
feature_cv <- feature_train %>% 
  vfold_cv(v = 10) # 10 cross validation folds

# so that this doesnt take long to run, do in sequence
doParallel::registerDoParallel() #build trees in parallel

# Resampling 
system.time( # measure how long it takes to run
  bag_tree_rs <- tune_grid(
    wf_bag_tree_tune, # add workflow with model 
    resamples = feature_cv,
    grid = bag_tree_grid,
    metrics = metric_set(accuracy)
  )
)

# make a final workflow that selects the best values 
final_bag_tree <- finalize_workflow(wf_bag_tree_tune, 
                                select_best(bag_tree_rs))

# fit the model to test data
final_bag_tree_result <- last_fit(final_bag_tree,
                              feature_split) # use split

```
```{r eval = FALSE, include = TRUE}
# plot the accuracy
bag_tree_rs %>% autoplot() +
  theme_bw()

## compare these to the way you did the tree above
final_bag_tree_result$.predictions

# the testing accuracy and auc for the test data
final_bag_tree_result$.metrics
```
    

### 4.  random forest (Week 6)
    -   rand_forest()
    -   m_try() is the new hyperparameter of interest for this type of model. Make sure to include it in your tuning process

```{r}
set.seed(279)
feature_rf <- rand_forest(mtry = tune(),
                       trees = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# create workflow with feature recipe
rf_workflow <- workflow() %>% 
  add_model(feature_rf) %>% 
  add_recipe(feature_recipe)


# set cross validation and resampling  
rf_cv_tune <- rf_workflow %>% 
  tune_grid(resamples = cv_folds, # defined above using cv_folds = vfold_cv(train_data, v = 5) 
            metrics = metric_set(roc_auc),
            grid = 10) # 10 values for combinations of mtry and trees

# make a final workflow that selects the best values 
final_rf <- finalize_workflow(rf_workflow, # add workflow
                                select_best(rf_cv_tune)) # select the best metrics

# fit the model to test data
final_rf_result <- fit(final_rf, feature_train) # use training to fit model
rf_metrics <- last_fit(final_rf, feature_split) # use split

rf_predict <- predict(final_rf_result, feature_test) %>% 
  bind_cols(feature_test)

```

```{r eval = FALSE, include=TRUE}
# Below are the results of the random forest cross validation tuning the mtry and trees parameters.
collect_metrics(rf_cv_tune)

autoplot(rf_cv_tune) + #plot cv results for parameter tuning
  theme_bw()

final_rf_result

accuracy(rf_predict, truth = partner_id, estimate = .pred_class) #get accuracy of testing prediction
```


## Compare the performance of the four final models you have created.

```{r}
set.seed(279)

#knn model
knn_metrics <- as.data.frame(train_knn$.metrics) %>% 
  select(.metric, .estimate) %>% 
  pivot_wider(names_from = .metric, 
              values_from = .estimate) %>% 
  mutate(model = "knn")

# decision tree model
#final_tree_result$.predictions
dt_metrics <- as.data.frame(final_tree_result$.metrics) %>% 
  select(.metric, .estimate) %>%
  pivot_wider(names_from = .metric, 
              values_from = .estimate) %>% 
  mutate(model = "decision tree")

# bagged trees model
#final_bag_tree_result
#final_bag_tree_result$.predictions
bag_metrics <- as.data.frame(final_bag_tree_result$.metrics) %>% 
  select(.metric, .estimate) %>%
  pivot_wider(names_from = .metric, 
              values_from = .estimate) %>% 
  mutate(model = "Bag Tree")

# random forest model 
rf_metrics_df <- as.data.frame(rf_metrics$.metrics) %>% 
  select(.metric, .estimate) %>%
  pivot_wider(names_from = .metric, 
              values_from = .estimate) %>% 
  mutate(model = "Random Forest")


```

### Make a table with the accuracy for each model 

Use appropriate performance evaluation metric(s) for this classification task. A table would be a good way to display your comparison. Use at least one visualization illustrating your model results.
```{r}
compare_models <- rbind(knn_metrics, dt_metrics, bag_metrics, rf_metrics_df)

sjPlot::tab_df(compare_models)
```

Looking at the confusion matrix for the best performing model: Random Forest:
```{r}

rf_cf <- confusionMatrix(data=rf_predict$.pred_class, reference = rf_predict$partner_id)
rf_cf

table(data=rf_predict$.pred_class, reference = rf_predict$partner_id)

```

