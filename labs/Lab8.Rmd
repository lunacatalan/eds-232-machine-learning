---
title: "Lab 8"
author: "Luna Herschenfeld-Catal√°n"
date: "2024-03-06"
output: html_document
---

## Forest Cover Classification with SVM

In this week's lab we are exploring the use of Support Vector Machines for multi-class classification. Specifically, you will be using cartographic variables to predict forest cover type (7 types).

Natural resource managers responsible for developing ecosystem management strategies require basic descriptive information including inventory data like forest cover type for forested lands to support their decision-making processes. However, managers generally do not have this type of data for in-holdings or neighboring lands that are outside their immediate jurisdiction. One method of obtaining this information is through the use of predictive models.

You task is build both an SVM and a random forest model and compare their performance on accuracy and computation time.

1.  The data is available here: <https://ucsb.box.com/s/ai5ost029enlguqyyn04bnlfaqmp8kn4>
```{r setup, message = FALSE}
library(tidymodels)
library(tidyverse)
library(dplyr)
library(kernlab)
library(tidyclust)

knitr::opts_chunk$set(message = FALSE, warning = FALSE)

set.seed(47)
```


Explore the data.

```{r}
covtype <- read_csv(here::here("labs", "data", "covtype_sample.csv")) %>% 
  janitor::clean_names() %>% 
  mutate(cover_type = as.factor(cover_type)) %>% 
  mutate(across(11:54, as.factor)) %>%   # convert binary to initial
  select(-soil_type_15, -soil_type_37, -soil_type_38, -soil_type_39, -soil_type_40)
  #select_if(function(col) length(unique(col))>1) %>%  # remove columns with only 0 in them

```

-   What kinds of features are we working with?

**Some of the features are continuous and some are categorical.**

-   Does anything stand out that will affect you modeling choices?

Hint: Pay special attention to the distribution of the outcome variable across the classes.

**Since there are both categorical and continuous variables, in our recipe we need to think critically about whether we are using a linear engine or a classification engine in the workflow. The `soil_type` variables are all categorical though, and it is coding for presence. When we plot the distribution of the `cover_type` which is the outcome variable, we also see that there is a large bias in the dataset. Most of the points are the `cover_type` 1 and 2, and `cover_type` 4 is very underrepresented.**

```{r}
ggplot(covtype) +
  geom_histogram(aes(x = cover_type), stat="count") +
  labs(title = "Distribution of Cover Types in Dataset") +
  theme_minimal() 
```


2.  Create the recipe and carry out any necessary preprocessing. Can you use the same recipe for both models?

**We can use the same recipe for both models because we are treating the data based on the attributes**

```{r}
# split the data 
cov_split <- initial_split(covtype)
cov_train <- training(cov_split)
cov_test <- testing(cov_split)

# create recipe with training data 
cov_recipe <- recipe(cover_type ~ ., data = cov_train) %>% 
  step_zv(all_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) # all the numeric variables

#centering and scale
#scale that up (look at the best parameters for that so that tuning isnt trying too much - use small data set)

```


3.  Create the folds for cross-validation.
```{r}
# create the folds 
vfolds <- vfold_cv(cov_train, strata = cover_type, v = 5)
```


4.  Tune the models. Choose appropriate parameters and grids. If the computational costs of tuning given your strategy are prohibitive, how might you work around this?

# SVM
```{r}
# create clustering 
svm_rbf_spec <- svm_rbf(cost = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")

# create workflow with feature recipe
svm_workflow <- workflow() %>% 
  add_model(svm_rbf_spec) %>% 
  add_recipe(cov_recipe)

svm_grid <- grid_regular(cost(), 
                         levels = 5)

doParallel::registerDoParallel(cores = 4)

system.time(
  svm_tune <- tune_grid(
    svm_workflow,
    resamples = vfolds,
    grid = svm_grid
  )
)

autoplot(svm_tune)

best_cost <- select_best(svm_tune, 
                         metric = "accuracy")

svm_final <- finalize_workflow(svm_workflow, 
                                      best_cost)

svm_fit <- svm_final %>% 
  fit(cov_train)

svm_rbf_augment1 <- augment(svm_fit, new_data = cov_test)

# confusion matrix
svm_rbf_augment1 %>% 
  conf_mat(truth = cover_type, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

# roc curve
svm_rbf_augment1 %>%
  roc_curve(truth = cover_type, .pred_1, .pred_2, .pred_3, .pred_4, .pred_5, .pred_6, .pred_7) %>% 
  autoplot()

# roc curve
svm_rbf_augment1 %>%
  roc_auc(truth = cover_type, .pred_1, .pred_2, .pred_3, .pred_4, .pred_5, .pred_6, .pred_7)
```

## Second way 
```{r eval = FALSE}

# create clustering 
svm_rbf_spec <- svm_rbf() %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")

svm_rbf_fit <- svm_rbf_spec %>% 
  fit(cover_type ~ ., data = cov_train)

svm_rbf_augment <- augment(svm_rbf_fit, new_data = cov_train)

# confusion matrix
svm_rbf_augment %>% 
  conf_mat(truth = cover_type, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

# roc curve
svm_rbf_augment %>%
  roc_curve(truth = cover_type, .pred_1, .pred_2, .pred_3, .pred_4, .pred_5, .pred_6, .pred_7) %>% 
  autoplot()

# roc curve
svm_rbf_augment %>%
  roc_auc(truth = cover_type, .pred_1, .pred_2, .pred_3, .pred_4, .pred_5, .pred_6, .pred_7)

# accuracy curve
svm_rbf_augment %>%
  accuracy(truth = cover_type, estimate = .pred_class)

```



# Random Forest
```{r}
rf_spec <- rand_forest(mtry = tune(), 
                        trees = tune()) %>%
  set_engine("ranger") %>% 
  set_mode("classification") # the output is cover type

# create workflow with feature recipe
rf_workflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(cov_recipe)

# this is taking a r
rf_tune <- rf_workflow %>% 
  tune_grid(
    resamples = vfolds, # add folds
    grid = 5 # number of combos of mtry and trees
  )

```


5.  Conduct final predictions for both models and compare their prediction performances and computation costs from part 4.

```{r}
rf_final = finalize_workflow(rf_workflow, 
                             select_best(rf_tune, metric = "roc_auc"))

rf_fit <- fit(rf_final, cov_train) # fit the data to the training data

train_predict <- predict(object = rf_fit, new_data = cov_train) %>% # predict the training set
  bind_cols(cov_train) # bind training set column to prediction

test_predict <- predict(object = rf_fit, new_data = cov_test) %>% # predict the training set
  bind_cols(cov_test) # bind prediction to testing data column

train_metrics <- train_predict %>%
  metrics(cover_type, .pred_class) # get testing data metrics

test_metrics <- test_predict %>%
  metrics(cover_type, .pred_class) # get testing data metrics

train_metrics
test_metrics
```


-   Which type of model do you think is better for this task?

Both models performed with similar accuracy, but the random forest performs with about 5% more accuracy than the svm model. Also, computationally the random forest was a bit faster, so I would select that one to do this task. 

-   Why do you speculate this is the case?

This is probably the case because there is a class imablance and the random forest is better at handling that. The random forest is built to handle class differences, and the svm is built for binary (two-class). Since we have an imbalance in the distribution of each characteristic, the random forest is better for handing this.
