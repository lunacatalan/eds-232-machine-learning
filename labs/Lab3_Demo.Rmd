---
title: "Lab 3 Demo"
author: "Luna Herschenfeld-catalan"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(skimr) # exploratory package for visualizing; summaries; distributions
library(glmnet) # use to estimate 
```

## Data Wrangling and Exploration
```{r data}
#load and inspect the data
dat <- AmesHousing::make_ames()

```

##Train a model
```{r intial_split}
# Data splitting with {rsample} 
set.seed(123) #set a seed for reproducibility

# initial split of data; default is 70/30
split <- initial_split(dat)
split

ames_train <- training(split)  
ames_test  <- testing(split)

```

```{r model_data}
#Create training feature matrices using model.matrix() (auto encoding of categorical variables)

X <- model.matrix(Sale_Price ~ ., data = ames_train)[,-1] # make all variables predictors

# transform y with log() transformation
Y <- log(ames_train$Sale_Price)

```

Why log() trainsform it?
Use `skim()` in the console to see what the distribution of the variable is. It looks really skewed!

```{r glmnet}
#fit a ridge model, passing X,Y,alpha to glmnet()
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0 # tells if you want function to be ridge (0), lasso (1)
)

#plot() the glmnet model object
# want to see how much of a labda do we want to impose on the coefficients 
plot(ridge, xvar = "lambda")  
```
Each line is a feature that impacts Sales_price. Lambda's job is to shrink coefficients, and as it increases they get to 0 so it is doing its job!


```{r}
# lambdas applied to penalty parameter.  Examine the first few
ridge$lambda %>% 
  head()


# small lambda results in large coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), # see the coefficient on the model 
            100] # look at the 100th row

# what about for small coefficients?
# make the coefficients much smaller on the variables of interest
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), # see the coefficient on the model 
            1] # look at the 1st row of the plot

  
```

How much improvement to our loss function as lambda changes?

##Tuning

Cross validation: how well the mdoel will perform on unseen data. This is splitting the training data 
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()
# cross validation for glmnet model
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0 # set up a ridge
)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1 # set up a lasso
)
  
# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```
Looking at the mean squared errors to see how well the model is predicting the split training data. As lambda is chaining, the mean squared error is changing. Want the mean squared error to be low for a GOOD model. This looks like as lambda gets larger, the model performs worse. 
- The first dotted line: which value to lambda gives you the lowest mean squared error. 
- The second dotted line: 1 standard error rule. There is a trade-off between MSE and parsimony in the model (model with fewer variables). Don't want to just rely on the MSE. This line identifies where the model is most parsimoneous (aka has the fewest number of variables), since how effective the estimate of MSE is varies. 


10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard deviation"?

In both models we see a slight improvement in the MSE as our penalty log(Î») gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

Let's examine the important parameter values apparent in the plots.
```{r}
# Ridge model
# minimum MSE
min(ridge$cvm)

# lambda for this min MSE
ridge$lambda.min

# 1-SE rule
# look at the cross validation model coefficient when lambda is equal to the 1se
ridge$cvm[ridge$lambda == ridge$lambda.1se]

# lambda for this MSE
ridge$lambda.1se

# ---------------------------
# Lasso model
# ---------------------------

# minimum MSE
min(lasso$cvm)

# lambda for this min MSE
lasso$lambda.min

# 1-SE rule
lasso$cvm[lasso$lambda == lasso$lambda.1se]

# lambda for this MSE
lasso$lambda.1se

# No. of coef | 1-SE MSE
# the lass model does feature selection, so the number of coef might end up being different 
# nzero is the NUMBER OF VARIABLES TO ZERO
lasso$nzero[lasso$lambda == lasso$lambda.1se]
```
The number output below the sXX is the number of coefficients that are still in the model. When you make lambda equal to `lasso$lambda.1se` then you are looking at the number of coefficients 1sd away from the MSE, which reduces the number of coefficients to 75. 


```{r}
# Ridge model
ridge_min 

# Lasso model
lasso_min


par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```

