---
title: "Lab6"
author: "Luna Herschenfeld-Catalán"
date: "2023-03-01"
output: html_document
---

## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab.  You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other.  Then you'll compare your model's performance to the model used by Elith et al.

## Data

Grab the training data sets (eel.model.data.csv, eel.eval.data.csv) from github here:
https://github.com/MaRo406/eds-232-machine-learning/blob/main/data 

```{r setup, message = FALSE}
library(spotifyr) #API interaction
library(tidyverse)
library(tidymodels)
library(parsnip)

set.seed(136)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.

```{r}

eel_model <- read_csv(here::here("labs", "data", "eel.model.data.csv")) %>% 
  select(-Site) %>% 
  mutate(Angaus = as.factor(Angaus),
         Method = as.factor(Method))

# Split the data
eel_split <- initial_split(eel_model, strata = Angaus)
eel_train <- training(eel_split)
eel_test <- testing(eel_split)

cv_folds = vfold_cv(eel_train, v = 10)

```


### Preprocess

Create a recipe to prepare your data for the XGBoost model
```{r}
eel_recipe <- recipe(Angaus ~ ., data = eel_train) %>% 
  step_dummy(all_nominal_predictors()) %>% #create dummy variables from all factors
  step_normalize(all_numeric_predictors()) #normalize all numeric predictors
```


## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}

xgb_model <- boost_tree(learn_rate = tune()) %>% 
  set_engine("xgboost") %>% # engine for boost trees
  set_mode("classification")

xgb_workflow <- workflow() %>% 
  add_model(xgb_model) %>% 
  add_recipe(eel_recipe)
```

2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

```{r}

# Resampling 
  #  user  system elapsed 
  # 8.587   0.083   8.747 
system.time(
  
  xgb_grid <- tune_grid(
    xgb_workflow, # add workflow
    resamples = cv_folds, # add folds
    grid = expand.grid(learn_rate = seq(0.0001, 0.3, 
                                        length.out = 30)),
    metrics = metric_set(roc_auc)
  )
)
```


3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}

# look at the metrics for each model
collect_metrics(xgb_grid)

autoplot(xgb_grid) + #plot cv results for parameter tuning
  theme_bw()

show_best(xgb_grid, n = 1, metric = "roc_auc")
```


### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.
```{r}
# learning rate: 0.2896586

xgb_model_2 <- boost_tree(learn_rate = 0.2896586,
                          tree_depth = tune(), # depth of tree
                          trees = 3000, # number of trees
                          loss_reduction = tune(),
                          min_n = tune()) %>%  # min number of trees
  set_engine("xgboost") %>% # engine for boost trees
  set_mode("classification")

xgb_workflow_2 <- workflow() %>% 
  add_model(xgb_model_2) %>% 
  add_recipe(eel_recipe)
```

2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space
- Figure this out!
```{r}

#extract_parameter_set_dials(xgb_workflow_2)

# Resampling 
  #  user  system elapsed 
  # 31.597   0.528  31.932
system.time(
  
  xgb_grid_2 <- tune_grid(
    xgb_workflow_2, # add workflow
    resamples = cv_folds, # add folds
    grid = grid_latin_hypercube(tree_depth(),
                                min_n(), 
                                loss_reduction()),
    metrics = metric_set(roc_auc)
  )
)

```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.
```{r}

# look at the metrics for each model
collect_metrics(xgb_grid_2)

autoplot(xgb_grid_2) + #plot cv results for parameter tuning
  theme_bw()

show_best(xgb_grid_2, n = 1, metric = "roc_auc")
```



### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.
```{r}

# learning rate: 0.2896586
# tree_depth: 14
# loss_reduction: 0.9827441
# min_n: 2

xgb_model_3 <- boost_tree(learn_rate = 0.2896586,
                          tree_depth = 11, # depth of tree
                          loss_reduction = 0.9827441,
                          mtry = tune(), # number of trees
                          sample_size = tune(),
                          stop_iter = tune(),
                          min_n = 2) %>%  # min number of trees
  set_engine("xgboost") %>% # engine for boost trees
  set_mode("classification")

xgb_workflow_3 <- workflow() %>% 
  add_model(xgb_model_3) %>% 
  add_recipe(eel_recipe)

```


2.  Set up a tuning grid. Use grid_latin_hypercube() again.
```{r}


# Resampling 
  #  user  system elapsed 
  # 15.872   0.350  16.471 

system.time(
  
  xgb_grid_3 <- tune_grid(
    xgb_workflow_3, # add workflow
    resamples = cv_folds, # add folds
    grid = grid_latin_hypercube(finalize(mtry(), 
                                         select(eel_train,-Angaus)),
                                size = 50,
                                sample_size = sample_prop(c(0.4, 0.9)),
                                stop_iter()),
    metrics = metric_set(roc_auc)
  )
)


```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}

# look at the metrics for each model
collect_metrics(xgb_grid_3)

autoplot(xgb_grid_3) + #plot cv results for parameter tuning
  theme_bw()

show_best(xgb_grid_3, n = 1, metric = "roc_auc")
```


## Finalize workflow and make final prediction
```{r}

# learning rate: 0.2896586
# tree_depth: 14
# loss_reduction: 0.9827441
# min_n: 2
# mtry: 13
# sample_size: 0.8665636
# stop_iter: 13


xgb_model_final <- boost_tree(learn_rate = 0.2896586,
                          tree_depth = 11, # depth of tree
                          loss_reduction = 0.9827441,
                          mtry = 13, # number of trees
                          sample_size = 0.8665636,
                          stop_iter = 13,
                          min_n = 2) %>%  # min number of trees
  set_engine("xgboost") %>% # engine for boost trees
  set_mode("classification")

xgb_workflow_final <- workflow() %>% 
  add_model(xgb_model_final) %>% 
  add_recipe(eel_recipe)


xgb_fit <- fit(xgb_workflow_final, eel_train)

test_predict_xgb = predict(xgb_fit, eel_test) %>% #get testing prediction
  bind_cols(eel_test)
```


1.  How well did your model perform? What types of errors did it make?

The model had a 83.66% accuracy rate when fit to the testing data, and an `roc_auc` score of 0.854.

```{r}

# fit the workflow to the whole data
xgb_last <- last_fit(xgb_workflow_final, eel_split)

# ways to evaluate accuracy
accuracy(test_predict_xgb, truth = Angaus, estimate = .pred_class) #get accuracy of testing prediction

xgb_last$.metrics
```

```{r}

test_predict_xgb %>% 
  conf_mat(truth = Angaus, estimate = .pred_class) %>% #create confusion matrix
  autoplot(type = "heatmap") + #plot confusion matrix with heatmap
  theme_bw() + #change theme
  theme(axis.text.x = element_text(angle = 30, hjust=1)) +
  #rotate axis labels
  labs(title = "XGB Confusion Matrix")
```

When we plot the confusion matrix, we can see that the most common error that the model is making is false negatives (predicting a 0 when the truth is 1). In a couple of instances, the model is also producing false positices (predicting a 1 when the truth is 0). 

## Fit your model the evaluation data and compare performance

1.  Now used your final model to predict on the other dataset (eval.data.csv)
```{r}
eel_data <- read_csv(here::here("labs", "data", "eel.eval.data.csv")) %>% 
  mutate(Angaus_obs = as.factor(Angaus_obs),
         Method = as.factor(Method))

eval_predict_xgb = predict(xgb_fit, eel_data) %>%  #get testing prediction
  bind_cols(eel_data)
```


2.  How does your model perform on this data?
```{r}

accuracy(eval_predict_xgb, truth = Angaus_obs, estimate = .pred_class)
```
This model has an 82.8% accuracy rate on the evaluation data. 

3.  How do your results compare to those of Elith et al.?

The results of the Elith et al. indicated that the roc_auc (Area under the receiver operating characteristic curve) was 0.858. We got similar results. 

-   Use {vip} to compare variable importance

```{r}
vip::vip(xgb_fit)
```

-   What do your variable importance results tell you about the distribution of this eel species?

The most important variable is `SegSumT`, which has almost twice as much importance as the next most important variable (USNative). This means that the distribution of this eel species is very impacted by Summer air temperature (°C). 

