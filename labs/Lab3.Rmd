---
title: "HerschenfeldCatalan_Lab3"
author: 'Luna Herschenfeld-Catalan'
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(glmnet)
library(patchwork)
library(sjPlot)
```

## Lab 3: Predicting the age of abalone

Abalones are marine snails. Their flesh is widely considered to be a desirable food, and is consumed raw or cooked by a variety of cultures. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age.

The data set provided includes variables related to the sex, physical dimensions of the shell, and various weight measurements, along with the number of rings in the shell. Number of rings is the stand-in here for age.

### Data Exploration

Pull the abalone data from Github and take a look at it.

```{r data}
abdat<- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/abalone-data.csv")
glimpse(abdat)

```

### Data Splitting

-   ***Question 1***. Split the data into training and test sets. Use a 70/30 training/test split.

We'll follow our text book's lead and use the caret package in our approach to this task. We will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. In particular, we must pass in an x matrix of predictors as well as a y outcome vector , and we do not use the yâˆ¼x syntax.

```{r}
set.seed(123)
# do the inital split
ab_split <- initial_split(abdat,
                          prop = 0.7)

# split data into test and training data 
ab_train <- training(ab_split)
ab_test <- testing(ab_split)
```

### Fit a ridge regression model

-   ***Question 2***. Use the model.matrix() function to create a predictor matrix, x, and assign the Rings variable to an outcome vector, y.

```{r}

X <- model.matrix(Rings ~ ., # make all variables predictors
                  data = ab_train)[,-1] 

# assign Predictor variable
Y <- ab_train$Rings

```


-   ***Question 3***. Fit a ridge model (controlled by the alpha parameter) using the glmnet() function. Make a plot showing how the estimated coefficients change with lambda. (Hint: You can call plot() directly on the glmnet() objects).

```{r}
#fit a ridge model, passing X,Y,alpha to glmnet()
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0 # rigde function
) %>% 
  plot(xvar = "lambda") # plot with lambda on x axis
```


### Using *k*-fold cross validation resampling and tuning our models

In lecture we learned about two methods of estimating our model's generalization error by resampling, cross validation and bootstrapping. We'll use the *k*-fold cross validation method in this lab. Recall that lambda is a tuning parameter that helps keep our model from over-fitting to the training data. Tuning is the process of finding the optima value of lambda.

-   ***Question 4***. This time fit a ridge regression model and a lasso model, both with using cross validation. The glmnet package kindly provides a cv.glmnet() function to do this (similar to the glmnet() function that we just used). Use the alpha argument to control which type of model you are running. Plot the results.
```{r}
par(mfrow = c(1, 2))

# cross validation 
ab_ridge <- cv.glmnet( 
  x = X,
  y = Y,
  alpha = 0
)

# plot ridge regression
ab_ridge %>% 
  plot(main = "Ridge penalty")

# cross validation 
ab_lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
) 

# plot lasso regression
ab_lasso %>% 
  plot(main = "Lasso penalty")

```


-   ***Question 5***. Interpret the graphs. What is being displayed on the axes here? How does the performance of the models change with the value of lambda?

The graphs are showing model performance as a function of lambda. On the x axis, the graph is showing the value of lambda. On the y-axis is Mean-squared error (MSE). For both graphs, as lambda increases, the MSE increases, and therefore the model performance decreases. The first dotted line represents the value of lambda that gives you the lowest mean squared error. The second dotted line identifies the value of lambda where the model has the fewest number of variables. 

For the Ridge penalty model, the lambda value that minimizes MSE and where the model has the fewest number of variables is -1. For the Lasso penalty model that minimizes the MSE and has the fewest number of variables is a large range between -3 and less than -7. 

-   ***Question 6***. Inspect the ridge model object you created with cv.glmnet(). The \$cvm column shows the MSEs for each CV fold. What is the minimum MSE? What is the value of lambda associated with this MSE minimum?

```{r}

# minimum MSE 
min_mse <- min(ab_ridge$cvm)

# print lambda when MSE is minimized
min_lam <- ab_ridge$lambda.min

```
The minimum MSE for the ridge model is `r {min_mse}` and the lambda value associated with the MSE minimum is `r {min_lam}`.

-   ***Question 7***. Do the same for the lasso model. What is the minimum MSE? What is the value of lambda associated with this MSE minimum?

```{r}
# minimum MSE 
min_mse_2 <- min(ab_lasso$cvm)

# print lambda when MSE is minimized
min_lam_2 <- ab_lasso$lambda.min
```

The minimum MSE for the lasso model is `r {min_mse_2}` and the lambda value associated with the MSE minimum is `r {min_lam_2}`.

Data scientists often use the "one-standard-error" rule when tuning lambda to select the best model. This rule tells us to pick the most parsimonious model (fewest number of predictors) while still remaining within one standard error of the overall minimum cross validation error. The cv.glmnet() model object has a column that automatically finds the value of lambda associated with the model that produces an MSE that is one standard error from the MSE minimum (\$lambda.1se).

-   ***Question 8.*** Find the number of predictors associated with this model (hint: the \$nzero is the \# of predictors column).
```{r}
# when looking at the predictors column, select the lambda that produces MSE 1se from the minimum
lasso_pred <- ab_lasso$nzero[ab_lasso$lambda == ab_lasso$lambda.1se]

# when looking at the predictors column, select the lambda that produces MSE 1se from the minimum
ridge_pred <- ab_ridge$nzero[ab_ridge$lambda == ab_ridge$lambda.1se]
```

The number of predictors in the lasso model is `r {lasso_pred }`, which is less than the total number because it performs selection on the predictors to minimize the number needed in the model. The number of predictors in the ridge model is `r {ridge_pred}`, and it includes all the columns in the data set. This makes sense since ridge regression does not perform feature selection and keeps all predictors. 

-   ***Question 9*****.** Which regularized regression worked better for this task, ridge or lasso? Explain your answer.

For this task, the lasso model works slightly better for predicting the age of abalone. This can be determined by comparing the MSE for the ridge and lasso. The lasso model has a smaller MSE output (lasso: `r {min_mse_2}` vs ridge: `r {min_mse}`). Also, the lasso uses less coefficients (6 vs 10), which makes it a simpler model. The more simple model and the lower MSE makes the lasso better than the ridge model to predict the age of abalone.

Attempt at running model on test data abd evaluatin results:
```{r eval = FALSE}
set.seed(1)

X_test <- model.matrix(Rings ~ ., data = ab_test)[,-1] # make all variables predictors

# assign Predictor variable
Y_test <- ab_test$Rings

# cross validation for lasso 
bestlam_lasso <- ab_lasso$lambda.min # Select lamda that minimizes training MSE
lasso_pred <- predict(ab_lasso, s = bestlam_lasso, newx = X_test) # Use best lambda to predict test data
lasso_mse <- mean((lasso_pred - Y_test)^2) # Calculate test MSE

# cross validation for ridge 
bestlam_ridge <- ab_ridge$lambda.min # Select lamda that minimizes training MSE
ridge_pred <- predict(ab_ridge, s = bestlam_ridge, newx = X_test) # Use best lambda to predict test data
ridge_mse <- mean((ridge_pred - Y_test)^2) # Calculate test MSE

```

