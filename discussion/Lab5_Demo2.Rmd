---
title: "Lab5_Demo2"
author: "Mateo Robbins"
date: "2023-02-15"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(vip) #variable importance
```
## R
```{r}
kaggle_dat <- read_csv("genres_v2.csv")
unique(kaggle_dat$genre)
table(kaggle_dat$genre)

#Removing inappropriate columns and selecting trap and Hiphop as the two genres here and making case consistent

genre_dat <- kaggle_dat %>%
  select(-c(type, uri, track_href, analysis_url, `Unnamed: 0`, title, tempo, id, song_name)) %>%
  filter(genre == "Hiphop"|genre == "Rap") %>%
  mutate(genre = str_replace(genre, "Hiphop", "hiphop")) %>%
  mutate(genre = str_replace(genre, "Rap", "rap")) %>%
  mutate(genre = as.factor(genre))
```

```{r}
set.seed(437)
##split the data
genre_split <- initial_split(genre_dat)

genre_train <- training(genre_split)
genre_test <- testing(genre_split)
```

```{r recipe}
#Preprocess the data
genre_rec <- recipe(genre ~ ., data = genre_train) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_normalize(all_numeric_predictors()) # scaling and centering them
  
```

Set up a decision tree specification. Note: the cost_complexity parameter is a pruning penalty parameter that controls how much we penalize the number of terminal nodes in the tree.  It's conceptually similar to lambda from regularized regression.

If you set the parameters, you do not need to do cross validation because you have already selected the values. Therefore, you can go straight to fitting to the training data. However, if you tune(), then you need to use cross validation to select the best variables. 
```{r tree_specification}

# you can use tune() but this is specifying the parameter values 
tree_spec_fixed <- decision_tree(
  cost_complexity = 0.1, # the cost complexisty 
  tree_depth = 4, 
  min_n = 11
) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
```

But, as usual, we don't want just any old values for our hyperparameters, we want optimal values.
```{r}
#new spec, tell the model that we are tuning hyperparams
tree_spec_tune <- decision_tree(
  cost_complexity =tune(), # the cost complexisty 
  tree_depth = tune(), 
  min_n = tune()
) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

# to tune; need grid of parameters that are going to be in the model 
tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(),
                          levels = 5) # get 5 combos (5x5x5) of the parameters
```

```{r workflow_tree}
wf_tree_tune <- workflow() %>% 
  add_recipe(genre_rec) %>% 
  add_model(tree_spec_tune)
  
```

```{r resampling}
#set up k-fold cv. This can be used for all the algorithms
genre_cv <- genre_train %>% 
  vfold_cv(v = 10) # 10 cross validation folds

```

```{r}
# so that this doesnt take long to run, do in sequence
doParallel::registerDoParallel() #build trees in parallel
#200s

# system.time( # measure how long it takes to run
# tree_rs <- tune_grid(
#   tree_spec_tune, # add the tune() model
#   genre ~ .,
# )
# )

system.time( # measure how long it takes to run
  tree_rs <- tune_grid(
    wf_tree_tune, # add workflow with model 
    resamples = genre_cv,
    grid = tree_grid,
    metrics = metric_set(accuracy)
  )
)

tree_rs
```
Use autoplot() to examine how different parameter configurations relate to accuracy

The three variables are displayed across these plots. There are 5 graphs because there are 5 levels in the grid parameter we set above. 
```{r}
tree_rs %>% autoplot() +
  theme_bw()
```

```{r select_hyperparam}
show_best(tree_rs, n = 3)
```

We can finalize the model specification where we have replaced the tune functions with optimized values.

```{r final_tree_spec}
# make a final workflow that selects the best values 
final_tree <- finalize_workflow(wf_tree_tune, 
                                select_best(tree_rs))

final_tree
```

This model has not been fit yet though.

```{r final_tree_fit}
#similar functions here.
final_tree_fit <-  fit(final_tree, 
                       data = genre_train)

# last_fit() which fits on the training data and also evaluates on the test data 

final_tree_result <- last_fit(final_tree,
                              genre_split) # use split

final_tree_result$.predictions

# the testing accuracy and auc for the test data
final_tree_result$.metrics
```

#Visualize variable importance: How important are the variables for determining the prediction? Not all variables contribute equally. 
```{r tree_vip}
final_tree_fit %>% 
  vip(geom = "col",
      aesthetics = list(fill = "midnightblue", alpha = 0.8)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_bw()

```


